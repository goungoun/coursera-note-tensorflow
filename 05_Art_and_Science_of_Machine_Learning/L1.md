# Regularization for sparsity (L1)

## L1 vs L2
- L2 regularization only makes weights small, not zero
- L1 regularization zeroing out coefficient

## L1 Performance
- Fewer coefficients to store/load => Reduce memory, model size
- Fewer multiplications needed => Increase prediction speed
- Feature crosses lead to lots of input nodes, so having zero weights is especially important

## Elastic nets
- combine the feature selection of L1 regularization with the generalizability of L2 regularization